from openai import OpenAI
from typing import List, Dict
from core.prompt import system_prompt
from pydantic import BaseModel, Field
import transformers
import torch

client = OpenAI()

class HallucinationVerdict(BaseModel):
    is_hallucination: bool = Field(
        description="True if the following fact is not supported by the context, False otherwise."
    )

class FactScoreEvaluator:
    def __init__(self, model: str = "gpt-5-mini", local: bool = False):
        self.model = model
        self.local = local
        self.pipeline = None
        
        if self.local:
            self._init_local_model()
    
    def _init_local_model(self):
        """Initialize the local Mistral model for inference."""
        try:
            model_id = "mistralai/Mistral-7B-Instruct-v0.2"
            self.pipeline = transformers.pipeline(
                "text-generation",
                model=model_id,
                model_kwargs={
                    "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
                    "low_cpu_mem_usage": True,
                },
                device_map="auto",
            )
        except Exception as e:
            print(f"Error initializing local model: {e}")
            self.pipeline = None
    
    def _call_local_model(self, prompt: str, max_tokens: int = 256) -> str:
        """Call the local Mistral model."""
        if self.pipeline is None:
            raise Exception("Local model not initialized.")
        
        formatted_prompt = f"[INST] {prompt} [/INST]"
        outputs = self.pipeline(
            formatted_prompt,
            max_new_tokens=max_tokens,
            do_sample=False,
            temperature=0.0,
            return_full_text=False,
        )
        return outputs[0]['generated_text'].strip()
    
    def _get_atomic_facts(self, response: str) -> List[str]:
        """
        Extract atomic facts from the model's response.
        Args:
            response: The response string from the language model.
        Returns:
            A list of atomic facts.
        """
        prompt = f"""Please breakdown the following sentence into independent facts: "{response}". """
        
        prompt_local = f"""Please breakdown the following sentence into independent facts: "{response}". List each fact on a new line."""
        if self.local:
            result = self._call_local_model(prompt_local, max_tokens=256)
        else:
            system_content = []
            system_text = system_prompt()
            system_content.append({"type": "input_text", "text": system_text})
            
            api_response = client.responses.create(
                model=self.model,
                input=[
                    {"role": "system", "content": system_content},
                    {"role": "user", "content": prompt}
                ]
            )
            result = api_response.output_text

        raw_facts = result.strip().split('\n')
        facts = [fact.strip('- ').strip() for fact in raw_facts if fact.strip()]

        return facts
    
    def _check_fact_support(self, fact: str, context: str) -> bool:
        """
        Check if a given fact is supported by the provided context.
        Args:
            fact: The atomic fact to check.
            context: The context string to verify against.
        Returns:
            True if the fact is supported by the context, False otherwise.
        """
        prompt = f"""Is the following fact supported by the context? 
        Fact: "{fact}"
        Context: "{context}"

        Answer False if supported, or True if not supported.
        """

        prompt_local = f"""Is the following fact supported by the context?
        Fact: "{fact}"
        Context: "{context}"
        Answer ONLY "Yes" if supported, or "No" if not supported.
        """
        
        if self.local:
            result = self._call_local_model(prompt_local, max_tokens=10)
            answer_lower = result.lower()
            is_supported = "yes" in answer_lower or "sim" in answer_lower
            return is_supported
        else:
            response = client.responses.parse(
                model=self.model,
                input=[
                    {"role": "user", "content": prompt}
                ],
                text_format=HallucinationVerdict
            )
            answer = response.output_parsed.is_hallucination

            #print(f"Fact: {fact} | Supported: {not answer}")
            return not answer
    
    def calculate_score(self, generated_answer: str, retrieved_context: str) -> Dict:
        """
        Calculate the factual accuracy score of the generated answer based on the retrieved context.
        Args:
            generated_answer: The answer generated by the language model.
            retrieved_context: The context used to generate the answer.
        Returns:
            A dictionary with total facts, supported facts, unsupported facts, and the factual accuracy score.
        """
        atomic_facts = self._get_atomic_facts(generated_answer)
        total_facts = len(atomic_facts)
        supported_facts = 0

        for fact in atomic_facts:
            if self._check_fact_support(fact, retrieved_context):
                supported_facts += 1

        unsupported_facts = total_facts - supported_facts
        factual_accuracy_score = supported_facts / total_facts if total_facts > 0 else 0.0

        return {
            "total_facts": total_facts,
            "supported_facts": supported_facts,
            "unsupported_facts": unsupported_facts,
            "factual_accuracy_score": factual_accuracy_score
        }